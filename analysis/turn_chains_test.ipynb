{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from scipy.signal import filtfilt\n",
    "\n",
    "\n",
    "# find all files in the directory that start with 'logs_' and end with '.hdf5'\n",
    "def get_hdf5_files(directory):\n",
    "    import os\n",
    "    import re\n",
    "    files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if re.match(r'logs_\\d+\\.hdf5', file):\n",
    "            files.append(os.path.join(directory, file))\n",
    "    return files\n",
    "\n",
    "def get_reward_split(hdf5_file):\n",
    "\n",
    "    with h5py.File(hdf5_file, 'r') as f:\n",
    "        cfg = dict(f['env_variables'].attrs)\n",
    "\n",
    "        event_survived_preditor = f['event_survived_predator'][:]\n",
    "        event_captured_by_preditor = f['event_captured_by_predator'][:]\n",
    "        event_consumed_prey = f['event_consumed_prey'][:]\n",
    "        internal_state = f['internal_state'][:]\n",
    "        episode_return = f['episode_return'][()]\n",
    "    salt_reward = np.sum(internal_state[:, 2] * cfg['reward_salt_factor'])\n",
    "    prey_reward = np.sum(event_consumed_prey * cfg['reward_consumption'])\n",
    "    pred_reward = np.sum(event_survived_preditor * cfg['reward_predator_avoidance'] +\n",
    "                       event_captured_by_preditor * cfg['reward_predator_caught'])\n",
    "    energy_changes = np.diff(internal_state[:, 1], prepend=internal_state[0, 1])\n",
    "    energy_losses = np.sum(energy_changes[energy_changes < 0])\n",
    "    energy_reward = energy_losses * cfg['reward_energy_use_factor']\n",
    "\n",
    "    reward_dict = {\n",
    "        'salt_reward': salt_reward,\n",
    "        'prey_reward': prey_reward,\n",
    "        'pred_reward': pred_reward,\n",
    "        'energy_reward': energy_reward,\n",
    "        'episode_return': episode_return\n",
    "    }\n",
    "    return reward_dict\n",
    "    \n",
    "def trig_actions(actions, trigs, seq_start, seq_end):\n",
    "    trig_ind = np.where(trigs)[0]\n",
    "    seqs = np.zeros((len(trig_ind), seq_end - seq_start))\n",
    "    for i, ind in enumerate(trig_ind):\n",
    "        start = max(0, ind + seq_start)\n",
    "        end = min(actions.shape[0], ind + seq_end)\n",
    "        seqs[i, :end - start] = actions[start:end]\n",
    "    return seqs\n",
    "\n",
    "def trig_fish_frame(fish_x, fish_y, fish_ori, obj_x, obj_y, trigs, offset=1):\n",
    "    trigs = trigs[offset:]\n",
    "    fish_x = fish_x[:-offset]\n",
    "    fish_y = fish_y[:-offset]\n",
    "    fish_ori = fish_ori[:-offset]\n",
    "    obj_x = obj_x[:-offset, :]\n",
    "    obj_y = obj_y[:-offset, :]\n",
    "    \n",
    "    fish_x = fish_x[trigs]\n",
    "    fish_y = fish_y[trigs]\n",
    "    fish_ori = fish_ori[trigs]\n",
    "    obj_x = obj_x[trigs, :]\n",
    "    obj_y = obj_y[trigs, :]\n",
    "    fish_ori = np.arctan2(np.sin(fish_ori), np.cos(fish_ori))\n",
    "    fish_prey_vectors = np.stack([(obj_x.T - fish_x).T, (obj_y.T - fish_y).T], axis=-1)\n",
    "\n",
    "    # rotate the vectors to the fish reference frame\n",
    "    c, s = np.cos(-fish_ori), np.sin(-fish_ori)\n",
    "    R = np.array([[c, -s], [s, c]])\n",
    "    fish_prey_vectors = fish_prey_vectors @ R.T\n",
    "\n",
    "\n",
    "    return fish_prey_vectors\n",
    "\n",
    "def read_events_file(events_file, tag):\n",
    "    event_acc = EventAccumulator(events_file, size_guidance={'tensors': 0})\n",
    "    event_acc.Reload()\n",
    "    res =np.array([[s, float(tf.make_ndarray(t))] for w, s, t in event_acc.Tensors(tag)])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir: /home/asaph/gcp_output/stage2_normal_nohdf5/stage2_1/logs/, Num streaks: 9816\n",
      "Dir: /home/asaph/gcp_output/stage2_normal_nohdf5/stage2_2/logs/, Num streaks: 10078\n",
      "Dir: /home/asaph/gcp_output/stage2_normal_nohdf5/stage2_3/logs/, Num streaks: 10074\n",
      "Dir: /home/asaph/gcp_output/stage2_normal_nohdf5/stage2_4/logs/, Num streaks: 10433\n",
      "Dir: /home/asaph/gcp_output/stage2_normal_nohdf5/stage2_5/logs/, Num streaks: 12001\n",
      "Dir: /home/asaph/gcp_output/stage2_normal_nohdf5/stage2_6/logs/, Num streaks: 10693\n",
      "Dir: /home/asaph/gcp_output/stage2_normal_nohdf5/stage2_7/logs/, Num streaks: 11436\n",
      "Dir: /home/asaph/gcp_output/stage2_normal_nohdf5/stage2_8/logs/, Num streaks: 10131\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "env_type = 'eval_normal'\n",
    "dirs = ['/home/asaph/gcp_output/stage2_normal_nohdf5/stage2_1/logs/',\n",
    "        '/home/asaph/gcp_output/stage2_normal_nohdf5/stage2_2/logs/',\n",
    "        '/home/asaph/gcp_output/stage2_normal_nohdf5/stage2_3/logs/',\n",
    "        '/home/asaph/gcp_output/stage2_normal_nohdf5/stage2_4/logs/',\n",
    "        '/home/asaph/gcp_output/stage2_normal_nohdf5/stage2_5/logs/',\n",
    "        '/home/asaph/gcp_output/stage2_normal_nohdf5/stage2_6/logs/',\n",
    "        '/home/asaph/gcp_output/stage2_normal_nohdf5/stage2_7/logs/',\n",
    "        '/home/asaph/gcp_output/stage2_normal_nohdf5/stage2_8/logs/',\n",
    "\n",
    "]\n",
    "\n",
    "mean_streaks = []\n",
    "sem_streaks = []\n",
    "actions_proportion = []\n",
    "training_episode_return = []\n",
    "q5 = []\n",
    "q6 = []\n",
    "q3 = []\n",
    "q4 = []\n",
    "q0 = []\n",
    "q1 = []\n",
    "q18 = []\n",
    "q19 = []\n",
    "pre_avoid_seq = []\n",
    "pre_consume_seq = []\n",
    "all_rewards = {\n",
    "    'salt_reward': [],\n",
    "    'prey_reward': [],\n",
    "    'pred_reward': [],\n",
    "    'energy_reward': [],\n",
    "    'episode_return': []\n",
    "}\n",
    "for dir in dirs:\n",
    "    agent_rewards = {\n",
    "    'salt_reward': [],\n",
    "    'prey_reward': [],\n",
    "    'pred_reward': [],\n",
    "    'energy_reward': [],\n",
    "    'episode_return': []\n",
    "    }\n",
    "    training_logs_dir = dir + 'training/actor_0/'\n",
    "    events_file = glob.glob(training_logs_dir + 'events*')[0]\n",
    "    training_episode_return.append(read_events_file(events_file, 'actor/EpisodeReturn'))\n",
    "    \n",
    "    files = get_hdf5_files(dir + env_type + '/')\n",
    "    start_dist = []\n",
    "    end_dist = []\n",
    "    episode_return = []\n",
    "    actor_steps = []\n",
    "    streaks = []\n",
    "    actions = []\n",
    "    for file in files:\n",
    "        this_rewards = get_reward_split(file)\n",
    "        for k in this_rewards.keys():\n",
    "            agent_rewards[k].append(this_rewards[k])\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            # get the keys of the file\n",
    "            ori = f['fish_angle'][:]\n",
    "            fish_x = f['fish_x'][:]\n",
    "            fish_y = f['fish_y'][:]\n",
    "            prey_x = f['prey_x'][:]\n",
    "            prey_y = f['prey_y'][:]\n",
    "            pred_x = f['predator_x'][:]\n",
    "            pred_x = pred_x[:, np.newaxis]\n",
    "            pred_y = f['predator_y'][:]\n",
    "            pred_y = pred_y[:, np.newaxis]\n",
    "            internal_state = f['internal_state'][:]\n",
    "            event_survived_preditor = f['event_survived_predator'][:]\n",
    "            event_consumed_prey = f['event_consumed_prey'][:]\n",
    "            action = f['action'][:]\n",
    "        if dir == dirs[5]:\n",
    "            q0.append(trig_fish_frame(fish_x, fish_y, ori, prey_x, prey_y, action==0))\n",
    "            q1.append(trig_fish_frame(fish_x, fish_y, ori, prey_x, prey_y, action==1))\n",
    "            q5.append(trig_fish_frame(fish_x, fish_y, ori, prey_x, prey_y, action==5))\n",
    "            q6.append(trig_fish_frame(fish_x, fish_y, ori, prey_x, prey_y, action==6))\n",
    "            q3.append(trig_fish_frame(fish_x, fish_y, ori, pred_x, pred_y, action==3))\n",
    "            q4.append(trig_fish_frame(fish_x, fish_y, ori, pred_x, pred_y, action==4))\n",
    "            q18.append(trig_fish_frame(fish_x, fish_y, ori, pred_x, pred_y, action==18))\n",
    "            q19.append(trig_fish_frame(fish_x, fish_y, ori, pred_x, pred_y, action==19))\n",
    "            pre_avoid_seq.append(trig_actions(action, event_survived_preditor>0, -20, 5))\n",
    "            pre_consume_seq.append(trig_actions(action, event_consumed_prey>0, -20, 5))\n",
    "        mean_sal_state = np.mean(internal_state[:, 2])\n",
    "        actions.append(action)\n",
    "        early_ori = ori[:350]\n",
    "        turn_angle = early_ori[1:] - early_ori[:-1]\n",
    "        # shuffle turn_angle\n",
    "        #turn_angle = np.random.permutation(turn_angle)\n",
    "        turn_dir = (turn_angle > 0)*2 - 1\n",
    "        # replace turn_dir with a random vector of 1s and -1s\n",
    "\n",
    "        #turn_dir = np.random.choice([-1, 1], size=len(turn_dir))\n",
    "\n",
    "        switch_points = np.where(turn_dir[1:] != turn_dir[:-1])[0] + 1\n",
    "        for s in switch_points:\n",
    "            if fish_x[s] > 500 and fish_y[s] > 500 and fish_x[s] < 3500 and fish_y[s] < 3500:\n",
    "                this_dir = turn_dir[s]\n",
    "                if s + 25 < len(turn_dir):\n",
    "                    streaks.append(np.cumsum(turn_angle[s+1:s+25] * this_dir))\n",
    "    actions = np.concatenate(actions)\n",
    "    actions_counts = np.bincount(action, minlength=21)\n",
    "    actions_proportion.append(actions_counts / np.sum(actions_counts))\n",
    "    for k in agent_rewards.keys():\n",
    "        all_rewards[k].append(np.array(agent_rewards[k]))\n",
    "\n",
    "    streaks = np.array(streaks)\n",
    "    # add a column of zeros to the beginning of streaks\n",
    "    streaks = np.hstack((np.zeros((streaks.shape[0], 1)), streaks))\n",
    "    mean_streak = np.mean(streaks, axis=0)\n",
    "    sem_streak = np.std(streaks, axis=0) / np.sqrt(streaks.shape[0])\n",
    "    mean_streaks.append(mean_streak)\n",
    "    sem_streaks.append(sem_streak)\n",
    "    print(f\"Dir: {dir}, Num streaks: {streaks.shape[0]}\")\n",
    "mean_streaks = np.array(mean_streaks)\n",
    "sem_streaks = np.array(sem_streaks)\n",
    "for k in all_rewards.keys():\n",
    "    all_rewards[k] = np.array(all_rewards[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(scalars: list[float], weight: float) -> list[float]:\n",
    "    \"\"\"\n",
    "    EMA implementation according to\n",
    "    https://github.com/tensorflow/tensorboard/blob/34877f15153e1a2087316b9952c931807a122aa7/tensorboard/components/vz_line_chart2/line-chart.ts#L699\n",
    "    \"\"\"\n",
    "    last = 0\n",
    "    smoothed = []\n",
    "    num_acc = 0\n",
    "    for next_val in scalars:\n",
    "        last = last * weight + (1 - weight) * next_val\n",
    "        num_acc += 1\n",
    "        # de-bias\n",
    "        debias_weight = 1\n",
    "        if weight != 1:\n",
    "            debias_weight = 1 - weight**num_acc\n",
    "        smoothed_val = last / debias_weight\n",
    "        smoothed.append(smoothed_val)\n",
    "\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_types = []\n",
    "ids = []\n",
    "with h5py.File('../actions_all_bouts_with_null.h5', 'r') as f:\n",
    "    for group_name in f.keys():\n",
    "        group = f[group_name]\n",
    "        action = {\n",
    "                    'name': group_name,\n",
    "                    'mean': group['mean'][:],\n",
    "                    'cov': group['cov'][:],\n",
    "                    'is_turn': group.attrs['is_turn'],\n",
    "                    'is_capture': group.attrs['is_capture'],\n",
    "                    'color': group.attrs['color']\n",
    "            }\n",
    "        if '_L' in action['name']:\n",
    "            action['color'] *= 1.1\n",
    "        if '_R' in action['name']:\n",
    "            action['color'] *= 0.9\n",
    "        action['color'] = np.clip(action['color'], 0, 1)\n",
    "        ids.append(group.attrs['id'])\n",
    "        action_types.append(action)\n",
    "        # sort actions by id\n",
    "action_types = [x for _, x in sorted(zip(ids, action_types), key=lambda pair: pair[0])]\n",
    "action_names = [action_types[i]['name'] for i in range(len(action_types))]\n",
    "action_colors = [action_types[i]['color'] for i in range(len(action_types))]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Episode Return (EMA smoothed)')"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for ds in range(8):\n",
    "    smoothed = smooth(training_episode_return[ds][:, 1], 0.99)[10:]\n",
    "    plt.plot(training_episode_return[ds][10:, 0]/1e6, smoothed, label=f'Dir {ds+1}')\n",
    "plt.xlabel('Actor Steps (millions)')\n",
    "plt.ylabel('Episode Return (EMA smoothed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Proportion of actions')"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre_avoid = np.concatenate(pre_avoid_seq)\n",
    "# plt.imshow((pre_avoid==3) | (pre_avoid==4))\n",
    "plt.figure()\n",
    "from matplotlib.colors import ListedColormap\n",
    "pre_con = np.concatenate(pre_consume_seq)\n",
    "pre_avoid = np.concatenate(pre_avoid_seq)\n",
    "my_cmap = ListedColormap(np.array(action_colors))\n",
    "time_base = np.arange(pre_con.shape[1]) - 20\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(pre_con, aspect='auto', cmap=my_cmap, extent=[time_base[0], time_base[-1], 0, pre_con.shape[0]], interpolation='nearest')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(time_base, np.mean((pre_con==3) | (pre_con==4), axis=0), label='O-bend', color=action_colors[3])\n",
    "plt.plot(time_base, np.mean((pre_con==5) | (pre_con==6), axis=0), label='J turn', color=action_colors[5])\n",
    "plt.plot(time_base, np.mean((pre_con==0) | (pre_con==1), axis=0), label='Capture', color=action_colors[0])\n",
    "plt.plot(time_base, np.mean((pre_con==18) | (pre_con==19), axis=0), label='HAT', color=action_colors[18])\n",
    "plt.xlim([-20, 5])\n",
    "plt.legend()\n",
    "plt.xlabel('Time steps relative to prey capture')\n",
    "plt.ylabel('Proportion of actions')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(pre_avoid, aspect='auto', cmap=my_cmap, extent=[time_base[0], time_base[-1], 0, pre_avoid.shape[0]], interpolation='nearest')\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(time_base, np.mean((pre_avoid==3) | (pre_avoid==4), axis=0), label='O-bend', color=action_colors[3])\n",
    "plt.plot(time_base, np.mean((pre_avoid==5) | (pre_avoid==6), axis=0), label='J turn', color=action_colors[5])\n",
    "plt.plot(time_base, np.mean((pre_avoid==0) | (pre_avoid==1), axis=0), label='Capture', color=action_colors[0])\n",
    "plt.plot(time_base, np.mean((pre_avoid==18) | (pre_avoid==19), axis=0), label='HAT', color=action_colors[18])\n",
    "plt.xlim([-20, 5])\n",
    "plt.xlabel('Time steps relative to predator avoidance')\n",
    "plt.ylabel('Proportion of actions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-100.0, 250.0)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq0 = np.concatenate(q0, axis=0)\n",
    "qq1 = np.concatenate(q1, axis=0)\n",
    "qq3 = np.concatenate(q3, axis=0)\n",
    "qq4 = np.concatenate(q4, axis=0)\n",
    "qq5 = np.concatenate(q5, axis=0)\n",
    "qq6 = np.concatenate(q6, axis=0)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(qq0[:, :, 1], qq0[:, :, 0], s=2, alpha=0.05,color='orange', label='Short Capture')\n",
    "plt.scatter(qq1[:, :, 1], qq1[:, :, 0], s=2, alpha=0.05,color='c', label='Long Capture')\n",
    "\n",
    "plt.scatter(qq5[:, :, 1], qq5[:, :, 0], s=2, alpha=0.05,color='r', label='Right J turn')\n",
    "plt.scatter(qq6[:, :, 1], qq6[:, :, 0], s=2, alpha=0.05,color='b', label='Left J turn')\n",
    "\n",
    "plt.scatter(qq3[:, :, 1], qq3[:, :, 0], s=3, alpha=0.3,color='m', label='Right O-bend')\n",
    "plt.scatter(qq4[:, :, 1], qq4[:, :, 0], s=3, alpha=0.3,color='g', label='Left O-bend')\n",
    "plt.scatter(0, 0, s=100, c='k', marker='x')\n",
    "# plt.axis('equal')\n",
    "\n",
    "plt.xlim(-200, 200)\n",
    "plt.ylim(-100, 250)\n",
    "# plt.legend(bbox_to_anchor=(1.02, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "a_prop = np.array(actions_proportion)\n",
    "# show as stacked bar plot\n",
    "labels = [str(i) for i in range(a_prop.shape[1])]\n",
    "x = np.arange(a_prop.shape[0])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "bottom = np.zeros(a_prop.shape[0])\n",
    "# set a color sequence\n",
    "\n",
    "for i in range(a_prop.shape[1]):\n",
    "    ax.bar(x, a_prop[:, i], bottom=bottom, label=action_names[i], color=action_colors[i])\n",
    "    bottom += a_prop[:, i]\n",
    "ax.set_xlabel('Agent')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Action Proportions Across Agents')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], loc='upper left', bbox_to_anchor=(1.02, 1), fontsize=8)\n",
    "\n",
    "bottom = np.zeros(a_prop.shape[0])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "for i, reward_type in enumerate(['prey_reward', 'pred_reward']):\n",
    "    ax.bar(x, np.mean(all_rewards[reward_type], axis=1), label=reward_type, bottom=bottom)\n",
    "    bottom += np.mean(all_rewards[reward_type], axis=1)\n",
    "bottom = np.zeros(a_prop.shape[0])\n",
    "for i, reward_type in enumerate(['salt_reward', 'energy_reward']):\n",
    "    ax.bar(x, np.mean(all_rewards[reward_type], axis=1), label=reward_type, bottom=bottom)\n",
    "    bottom += np.mean(all_rewards[reward_type], axis=1)\n",
    "\n",
    "# make a line plot of episode return on the same axis\n",
    "#ax.plot(x, np.mean(all_rewards['episode_return'], axis=1), color='k', marker='o', label='Total Return')\n",
    "# plot error bars for episode return\n",
    "ax.errorbar(x, np.mean(all_rewards['episode_return'], axis=1), yerr=np.std(all_rewards['episode_return'], axis=1), color='k', fmt='o', capsize=3, label='Total Return')\n",
    "# ax[1].scatter(x, np.mean(-1*all_rewards['energy_reward'], axis=1), color='k', marker='o', label='Energy Cost')\n",
    "# ax[1].scatter(x, np.mean(-1*all_rewards['salt_reward'], axis=1), color='grey', marker='o', label='Salt Cost')\n",
    "\n",
    "ax.set_xlabel('Agent')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "# bold horizontal line at y=0\n",
    "ax.axhline(0, color='k', linewidth=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7b4f223a0a60>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dataset in range(len(dirs)):\n",
    "   \n",
    "    plt.plot(mean_streaks[dataset], '-o', label=f'Ag {dataset+1}')\n",
    "    plt.fill_between(np.arange(len(mean_streaks[dataset])),\n",
    "                     mean_streaks[dataset] - sem_streaks[dataset],\n",
    "                     mean_streaks[dataset] + sem_streaks[dataset], alpha=0.3)\n",
    "\n",
    "plt.axhline(0, color='k', linestyle='--', label='Chance')\n",
    "plt.xlabel('Timestep after switch')\n",
    "plt.ylabel('Cumulative Turn Angle (rad)')\n",
    "plt.title('Cumulative Turn Angle After Direction Switch')\n",
    "plt.legend(fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
