{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from scipy.signal import filtfilt\n",
    "import seaborn as sns\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none' # To get editable text in Illustrator\n",
    "\n",
    "# find all files in the directory that start with 'logs_' and end with '.hdf5'\n",
    "def get_hdf5_files(directory):\n",
    "    import os\n",
    "    import re\n",
    "    files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if re.match(r'logs_\\d+\\.hdf5', file):\n",
    "            files.append(os.path.join(directory, file))\n",
    "    return files\n",
    "\n",
    "def get_action_energy(impulse_factor, angle_factor):\n",
    "    action_names, action_colors, action_types, action_is_turn, action_means = get_actions('../actions_all_bouts_with_null.h5')\n",
    "    action_means = np.array(action_means)\n",
    "    action_distances = action_means[:, 0]\n",
    "    action_angles = action_means[:, 1]\n",
    "    action_impulses = (action_distances * 10) * 0.34452532909386484\n",
    "    action_energy = action_impulses * impulse_factor + np.abs(action_angles) * angle_factor\n",
    "    return action_energy\n",
    "\n",
    "def get_exploratory_energy(hdf5_file):\n",
    "    action_names, action_colors, action_types, action_is_turn, action_means = get_actions('../actions_all_bouts_with_null.h5')\n",
    "    \n",
    "    with h5py.File(hdf5_file, 'r') as f:\n",
    "        cfg = dict(f['env_variables'].attrs)\n",
    "        action = f['action'][:]\n",
    "        internal_state = f['internal_state'][:]\n",
    "\n",
    "    action_energy = get_action_energy(cfg['energy_impulse_factor'], cfg['energy_angle_factor'])\n",
    "\n",
    "    exploratory_energy_use = np.zeros_like(action, dtype=float)\n",
    "    exploratory_energy_use[:] = np.nan\n",
    "    for action_id in range(len(action_types)):\n",
    "        if action_names[action_id] != 'LCS' and action_names[action_id] != 'SCS': # exclude capture swims\n",
    "            exploratory_energy_use[action == action_id] = action_energy[action_id]\n",
    "    energy = internal_state[:, 1]\n",
    "    nulls = action == 20 # null action\n",
    "    return exploratory_energy_use, energy, nulls\n",
    "\n",
    "def get_reward_split(hdf5_file):\n",
    "\n",
    "    with h5py.File(hdf5_file, 'r') as f:\n",
    "        cfg = dict(f['env_variables'].attrs)\n",
    "\n",
    "        event_survived_preditor = f['event_survived_predator'][:]\n",
    "        event_captured_by_preditor = f['event_captured_by_predator'][:]\n",
    "        event_consumed_prey = f['event_consumed_prey'][:]\n",
    "        internal_state = f['internal_state'][:]\n",
    "        episode_return = f['episode_return'][()]\n",
    "    salt_reward = np.sum(internal_state[:, 2] * cfg['reward_salt_factor'])\n",
    "    prey_reward = np.sum(event_consumed_prey * cfg['reward_consumption'])\n",
    "    pred_reward = np.sum(event_survived_preditor * cfg['reward_predator_avoidance'] +\n",
    "                       event_captured_by_preditor * cfg['reward_predator_caught'])\n",
    "    energy_changes = np.diff(internal_state[:, 1], prepend=internal_state[0, 1])\n",
    "    energy_losses = np.sum(energy_changes[energy_changes < 0])\n",
    "    energy_reward = energy_losses * cfg['reward_energy_use_factor']\n",
    "\n",
    "    reward_dict = {\n",
    "        'salt_reward': salt_reward,\n",
    "        'prey_reward': prey_reward,\n",
    "        'pred_reward': pred_reward,\n",
    "        'energy_reward': energy_reward,\n",
    "        'episode_return': episode_return\n",
    "    }\n",
    "    return reward_dict\n",
    "    \n",
    "def trig_actions(actions, trigs, seq_start, seq_end):\n",
    "    trig_ind = np.where(trigs)[0]\n",
    "    seqs = np.zeros((len(trig_ind), seq_end - seq_start))\n",
    "    for i, ind in enumerate(trig_ind):\n",
    "        start = max(0, ind + seq_start)\n",
    "        end = min(actions.shape[0], ind + seq_end)\n",
    "        seqs[i, :end - start] = actions[start:end]\n",
    "    return seqs\n",
    "\n",
    "def trig_fish_frame(fish_x, fish_y, fish_ori, obj_x, obj_y, trigs, offset=1, lim=17):\n",
    "    trigs = trigs[offset:]\n",
    "    fish_x = fish_x[:-offset]\n",
    "    fish_y = fish_y[:-offset]\n",
    "    fish_ori = fish_ori[:-offset]\n",
    "    obj_x = obj_x[:-offset, :]\n",
    "    obj_y = obj_y[:-offset, :]\n",
    "    \n",
    "    fish_x = fish_x[trigs]\n",
    "    fish_y = fish_y[trigs]\n",
    "    fish_ori = fish_ori[trigs]\n",
    "    obj_x = obj_x[trigs, :]\n",
    "    obj_y = obj_y[trigs, :]\n",
    "    fish_ori = np.arctan2(np.sin(fish_ori), np.cos(fish_ori))\n",
    "    fish_prey_vectors = np.stack([(obj_x.T - fish_x).T, (obj_y.T - fish_y).T], axis=-1)\n",
    "\n",
    "    # rotate the vectors to the fish reference frame\n",
    "    c, s = np.cos(-fish_ori), np.sin(-fish_ori)\n",
    "    R = np.array([[c, -s], [s, c]])\n",
    "    fish_prey_vectors = fish_prey_vectors @ R.T\n",
    "\n",
    "    # limit to a square of size lim x lim\n",
    "    mask = (np.abs(fish_prey_vectors[..., 0]) < lim) & (np.abs(fish_prey_vectors[..., 1]) < lim)\n",
    "    fish_prey_vectors = fish_prey_vectors[mask]\n",
    "    return fish_prey_vectors\n",
    "\n",
    "def read_events_file(events_file, tag):\n",
    "    event_acc = EventAccumulator(events_file, size_guidance={'tensors': 0})\n",
    "    event_acc.Reload()\n",
    "    res =np.array([[s, float(tf.make_ndarray(t))] for w, s, t in event_acc.Tensors(tag)])\n",
    "    return res\n",
    "\n",
    "def get_turn_streaks(dir, length=25, turn_threshold=0.18):\n",
    "\n",
    "    files = get_hdf5_files(dir)\n",
    "    streaks = []\n",
    "    eligible_turns = 0\n",
    "    for file in files:\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            ori = f['fish_angle'][:]\n",
    "            fish_x = f['fish_x'][:]\n",
    "            fish_y = f['fish_y'][:]\n",
    "            cfg = dict(f['env_variables'].attrs)\n",
    "        turn_angle = ori[1:] - ori[:-1]\n",
    "        turn_angle_mask = np.abs(turn_angle) > turn_threshold\n",
    "        eligible_turns += np.sum(turn_angle_mask)\n",
    "        fish_x = fish_x[1:][turn_angle_mask]\n",
    "        fish_y = fish_y[1:][turn_angle_mask]\n",
    "        turn_angle = turn_angle[turn_angle_mask]\n",
    "        turn_dir = (turn_angle > 0)*2 - 1\n",
    "        switch_points = np.where(turn_dir[1:] != turn_dir[:-1])[0] + 1\n",
    "        for s in switch_points:\n",
    "            if fish_x[s] > 500 and fish_y[s] > 500 and fish_x[s] < (cfg['arena_width'] - 500) and fish_y[s] < (cfg['arena_height'] - 500):\n",
    "                this_dir = turn_dir[s]\n",
    "                if s + length < len(turn_dir):\n",
    "                    streaks.append(np.cumsum(turn_dir[s+1:s+length] * this_dir))\n",
    "    streaks = np.array(streaks)\n",
    "    # add a column of zeros to the beginning of streaks\n",
    "    streaks = np.hstack((np.zeros((streaks.shape[0], 1)), streaks))\n",
    "\n",
    "    return streaks, eligible_turns\n",
    "\n",
    "def get_reward_composition(dir_env):\n",
    "    files = get_hdf5_files(dir_env)\n",
    "    all_rewards = {\n",
    "        'salt_reward': [],\n",
    "        'prey_reward': [],\n",
    "        'pred_reward': [],\n",
    "        'energy_reward': [],\n",
    "        'episode_return': []\n",
    "    }\n",
    "    durations = []\n",
    "    for file in files:\n",
    "        agent_rewards = get_reward_split(file)\n",
    "        for k in agent_rewards.keys():\n",
    "            all_rewards[k].append(np.array(agent_rewards[k]))\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            durations.append(f['episode_length'][()])\n",
    "    durations = np.array(durations)\n",
    "    for k in all_rewards.keys():\n",
    "        all_rewards[k] = np.array(all_rewards[k])\n",
    "    return all_rewards, durations\n",
    "\n",
    "def get_dirs(training_env, eval_env):\n",
    "    dirs = [f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_1/logs/eval_{eval_env}/',\n",
    "            f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_2/logs/eval_{eval_env}/',\n",
    "            f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_3/logs/eval_{eval_env}/',\n",
    "            f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_4/logs/eval_{eval_env}/',\n",
    "            f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_5/logs/eval_{eval_env}/',\n",
    "            f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_6/logs/eval_{eval_env}/',\n",
    "            f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_7/logs/eval_{eval_env}/',\n",
    "            f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_8/logs/eval_{eval_env}/',\n",
    "    ]\n",
    "    return dirs\n",
    "\n",
    "def get_actions(actions_file):\n",
    "    action_types = []\n",
    "    ids = []\n",
    "    with h5py.File(actions_file, 'r') as f:\n",
    "        for group_name in f.keys():\n",
    "            group = f[group_name]\n",
    "            action = {\n",
    "                        'name': group_name,\n",
    "                        'mean': group['mean'][:],\n",
    "                        'cov': group['cov'][:],\n",
    "                        'is_turn': group.attrs['is_turn'],\n",
    "                        'is_capture': group.attrs['is_capture'],\n",
    "                        'color': group.attrs['color']\n",
    "                        \n",
    "                }\n",
    "            if '_L' in action['name']:\n",
    "                action['color'] *= 1.1\n",
    "            if '_R' in action['name']:\n",
    "                action['color'] *= 0.9\n",
    "            if 'Null' in action['name']:\n",
    "                action['color'] = np.array([0, 0, 0])\n",
    "            action['color'] = np.clip(action['color'], 0, 1)\n",
    "            ids.append(group.attrs['id'])\n",
    "            action_types.append(action)\n",
    "            # sort actions by id\n",
    "    action_types = [x for _, x in sorted(zip(ids, action_types), key=lambda pair: pair[0])]\n",
    "    action_names = [action_types[i]['name'] for i in range(len(action_types))]\n",
    "    action_colors = [action_types[i]['color'] for i in range(len(action_types))]  \n",
    "    action_is_turn = [action_types[i]['is_turn'] for i in range(len(action_types))]\n",
    "    action_means = [action_types[i]['mean'] for i in range(len(action_types))]\n",
    "    return action_names, action_colors, action_types, action_is_turn, action_means\n",
    "\n",
    "def get_trig_space(dir, lim=17):\n",
    "    files = get_hdf5_files(dir)\n",
    "    trig_space_prey = [np.zeros((0, 2)) for _ in range(21)]\n",
    "    trig_space_pred = [np.zeros((0, 2)) for _ in range(21)]\n",
    "    for file in files:\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            # get the keys of the file\n",
    "            ori = f['fish_angle'][:]\n",
    "            fish_x = f['fish_x'][:]\n",
    "            fish_y = f['fish_y'][:]\n",
    "            pred_x = f['predator_x'][:]\n",
    "            pred_x = pred_x[:, np.newaxis]\n",
    "            pred_y = f['predator_y'][:]\n",
    "            pred_y = pred_y[:, np.newaxis]\n",
    "            action = f['action'][:]\n",
    "            prey_x = f['prey_x'][:]\n",
    "            prey_y = f['prey_y'][:]\n",
    "            for action_id in range(21):\n",
    "                this_trig = trig_fish_frame(fish_x, fish_y, ori, prey_x, prey_y, action==action_id, lim=lim)\n",
    "                this_trig = np.reshape(this_trig, (-1, 2))\n",
    "                trig_space_prey[action_id] = np.concatenate((trig_space_prey[action_id], this_trig), axis=0)\n",
    "                this_trig = trig_fish_frame(fish_x, fish_y, ori, pred_x, pred_y, action==action_id, lim=lim)\n",
    "                this_trig = np.reshape(this_trig, (-1, 2))\n",
    "                trig_space_pred[action_id] = np.concatenate((trig_space_pred[action_id], this_trig), axis=0)\n",
    "    return trig_space_prey, trig_space_pred\n",
    "\n",
    "def get_sequences(dir, steps_pre=20, steps_post=5):\n",
    "    files = get_hdf5_files(dir)\n",
    "    avoid_seq = []\n",
    "    caught_seq = []\n",
    "    pred_seq = []\n",
    "    consume_seq = []\n",
    "    for file in files:\n",
    "        with h5py.File(file, 'r') as f:\n",
    "\n",
    "            event_survived_preditor = f['event_survived_predator'][:]\n",
    "            event_captured_by_preditor = f['event_captured_by_predator'][:]\n",
    "            event_consumed_prey = f['event_consumed_prey'][:]\n",
    "            pred_x = f['predator_x'][:]\n",
    "            # add initial zero to pred_x\n",
    "            pred_x = np.concatenate(([0], pred_x))\n",
    "\n",
    "            pred_events = np.where(np.diff((pred_x > 0).astype(int))>0)[0]\n",
    "            diffs = np.concatenate([[np.inf], np.diff(pred_events)])\n",
    "            pred_events = pred_events[diffs>np.max([steps_pre, steps_post])]\n",
    "            pred_event_vec = np.zeros_like(pred_x)\n",
    "            pred_event_vec[pred_events] = 1\n",
    "            action = f['action'][:]\n",
    "        avoid_seq.append(trig_actions(action, event_survived_preditor>0, -steps_pre, steps_post))\n",
    "        consume_seq.append(trig_actions(action, event_consumed_prey>0, -steps_pre, steps_post))\n",
    "        pred_seq.append(trig_actions(action, pred_event_vec>0, -steps_pre, steps_post))\n",
    "        caught_seq.append(trig_actions(action, event_captured_by_preditor>0, -steps_pre, steps_post))\n",
    "    avoid_seq = np.concatenate(avoid_seq)\n",
    "    caught_seq = np.concatenate(caught_seq)\n",
    "    pred_seq = np.concatenate(pred_seq)\n",
    "    consume_seq = np.concatenate(consume_seq)\n",
    "    return avoid_seq, caught_seq, pred_seq, consume_seq\n",
    "\n",
    "def plot_episode(file, arrows=False, prey_density=False):\n",
    "    action_names, action_colors, action_types, action_is_turn, _ = get_actions('../actions_all_bouts_with_null.h5')\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        cfg = dict(f['env_variables'].attrs)\n",
    "        fish_x = f['fish_x'][:]/10\n",
    "        fish_y = f['fish_y'][:]/10\n",
    "        fish_ori = f['fish_angle'][:]\n",
    "        prey_x = f['prey_x'][:]\n",
    "        prey_y = f['prey_y'][:]\n",
    "        pred_x = f['predator_x'][:]\n",
    "        pred_y = f['predator_y'][:]\n",
    "        internal_state = f['internal_state'][:]\n",
    "        actions = f['action'][:]\n",
    "        event_consumed_prey = f['event_consumed_prey'][:]\n",
    "\n",
    "        # make a square with size cfg['arena_wifht'] and cfg['arena_height']\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        w = cfg['arena_width']/10\n",
    "        h = cfg['arena_height']/10\n",
    "        if prey_density:\n",
    "            # show 2D KDE of initial prey locations\n",
    "            init_prey_x = prey_x[0, :]/10\n",
    "            init_prey_y = prey_y[0, :]/10\n",
    "            sns.kdeplot(x=init_prey_x, y=init_prey_y, fill=True, cmap=\"Blues\", thresh=0.5, bw_adjust=0.7, levels=30, alpha=0.5, antialiased=True)\n",
    "        \n",
    "        for step in range(1,len(actions)):\n",
    "            action = actions[step]\n",
    "            color = action_colors[action]\n",
    "            if arrows:\n",
    "                plt.arrow(fish_x[step-1], fish_y[step-1], (fish_x[step]-fish_x[step-1])*0.8, (fish_y[step]-fish_y[step-1])*0.8, head_width=1, head_length=1, fc=color, ec=color, linewidth=1, zorder=1)\n",
    "            else:\n",
    "                plt.plot(fish_x[step-1:step+1], fish_y[step-1:step+1], color=color, linewidth=1.5, zorder=1)\n",
    "        #plt.plot(fish_x, fish_y, '-b', label='Fish')\n",
    "        consumption_x = fish_x[event_consumed_prey==1]\n",
    "        consumption_y = fish_y[event_consumed_prey==1]\n",
    "        predator_present = np.where(np.diff(pred_x > 0))[0]\n",
    "        plt.scatter(fish_x[predator_present], fish_y[predator_present], c='red', label='Predator appears', s=40, edgecolors='k', zorder=10)\n",
    "        plt.scatter(consumption_x, consumption_y, c='green', label='Prey consumed', s=40, edgecolors='k', zorder=10)\n",
    "        # make a semi-transparent dark rectangle on the top fraction of the arena, according to cfg['arena_dark_fraction']\n",
    "        plt.fill_between([0, w], 0, h * (cfg['arena_dark_fraction']), color='k', alpha=0.5)\n",
    "        plt.plot([10, 10, 30], [200, 220, 220], '-k', linewidth=2, label='2 cm')\n",
    "        # invert the y axis\n",
    "        plt.xlim(0, w)\n",
    "        plt.ylim(0, h)\n",
    "        # remove ticks\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = '/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_1/logs/eval_sparse/logs_15.hdf5'\n",
    "action_names, action_colors, action_types, action_is_turn, action_means = get_actions('../actions_all_bouts_with_null.h5')\n",
    "    \n",
    "with h5py.File(hdf5_file, 'r') as f:\n",
    "    cfg = dict(f['env_variables'].attrs)\n",
    "    action = f['action'][:]\n",
    "    internal_state = f['internal_state'][:]\n",
    "\n",
    "action_energy = get_action_energy(cfg['energy_impulse_factor'], cfg['energy_angle_factor'])\n",
    "\n",
    "# show the relative action energies by plotting horizontal lines with corresponding colors at hight action_energy\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i in range(len(action_energy)):\n",
    "    plt.plot([0, 1], [action_energy[i], action_energy[i]], color=action_colors[i], linewidth=6)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, np.max(action_energy)*1.1)\n",
    "plt.ylabel('Action energy (a.u.)')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional exploratory energy (without fixing energy levels)\n",
    "\n",
    "conditional_explore_en = {\n",
    "    'normal': np.zeros((8, 10)),\n",
    "    'sparse': np.zeros((8, 10))\n",
    "\n",
    "}\n",
    "\n",
    "conditional_null_fraction = {\n",
    "    'normal': np.zeros((8, 10)),\n",
    "    'sparse': np.zeros((8, 10))\n",
    "}\n",
    "\n",
    "for env in ['normal', 'sparse']:\n",
    "    dirs = get_dirs(env, 'empty')\n",
    "\n",
    "    for dataset in range(len(dirs)):\n",
    "        print(f'Processing dataset {dataset+1}/{len(dirs)}')\n",
    "        all_explore_en = []\n",
    "        all_energy = []\n",
    "        all_nulls = []\n",
    "        for file in get_hdf5_files(dirs[dataset]):\n",
    "            explore_en, energy, nulls = get_exploratory_energy(file)\n",
    "            all_explore_en.append(explore_en)\n",
    "            all_energy.append(energy)\n",
    "            all_nulls.append(nulls)\n",
    "        all_explore_en = np.concatenate(all_explore_en)\n",
    "        all_energy = np.concatenate(all_energy)\n",
    "        all_nulls = np.concatenate(all_nulls)\n",
    "        # divide energy into 10 bins based on percentiles, calculate mean explore_en and null fraction in each bin\n",
    "        energy_bins = np.linspace(0, 1, 11)\n",
    "        energy_percentiles = np.percentile(all_energy, energy_bins * 100)\n",
    "        mean_explore_en = []\n",
    "        null_fractions = []\n",
    "        for i in range(len(energy_percentiles)-1):\n",
    "            in_bin = (all_energy >= energy_percentiles[i]) & (all_energy < energy_percentiles[i+1])\n",
    "            mean_explore_en.append(np.nanmean(all_explore_en[in_bin]))\n",
    "            null_fractions.append(np.sum(all_nulls[in_bin]) / np.sum(in_bin) if np.sum(in_bin) > 0 else np.nan)\n",
    "\n",
    "        conditional_explore_en[env][dataset] = np.array(mean_explore_en)\n",
    "        conditional_null_fraction[env][dataset] = np.array(null_fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "bins = np.linspace(0, 100, 10)\n",
    "plt.plot(bins, conditional_explore_en['normal'].T, color='C0', alpha=0.3)\n",
    "plt.plot(bins, conditional_explore_en['sparse'].T, color='C1', alpha=0.3)\n",
    "plt.plot(bins, np.mean(conditional_explore_en['normal'], axis=0), '-o', color='C0', linewidth=3, label='Dense')\n",
    "plt.plot(bins, np.mean(conditional_explore_en['sparse'], axis=0), '-o', color='C1', linewidth=3, label='Patchy')\n",
    "plt.xlabel('Energy percentile (%)')\n",
    "plt.ylabel('Exploratory energy (a.u.)')\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bins, conditional_null_fraction['normal'].T, color='C0', alpha=0.3)\n",
    "plt.plot(bins, conditional_null_fraction['sparse'].T, color='C1', alpha=0.3)\n",
    "plt.plot(bins, np.mean(conditional_null_fraction['normal'], axis=0), '-o', color='C0', linewidth=3, label='Dense')\n",
    "plt.plot(bins, np.mean(conditional_null_fraction['sparse'], axis=0), '-o', color='C1', linewidth=3, label='Patchy')\n",
    "plt.xlabel('Energy percentile (%)')\n",
    "plt.ylabel('Null action fraction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional exploratory energy (fixing energy levels)\n",
    "\n",
    "conditional_explore_en = {\n",
    "    'normal_empty_low_energy': np.zeros(8),\n",
    "    'normal_empty_high_energy': np.zeros(8),\n",
    "    'sparse_empty_low_energy': np.zeros(8),\n",
    "    'sparse_empty_high_energy': np.zeros(8)\n",
    "}\n",
    "for train_env in ['normal', 'sparse']:\n",
    "    for test_env in ['empty_low_energy', 'empty_high_energy']:\n",
    "        dirs = get_dirs(train_env, test_env)\n",
    "\n",
    "        for dataset in range(len(dirs)):\n",
    "            print(f'Processing dataset {dataset+1}/{len(dirs)}')\n",
    "            all_explore_en = []\n",
    "            for file in get_hdf5_files(dirs[dataset]):\n",
    "                explore_en, energy, nulls = get_exploratory_energy(file)\n",
    "                all_explore_en.append(explore_en)\n",
    "            all_explore_en = np.concatenate(all_explore_en)\n",
    "            conditional_explore_en[train_env + '_' + test_env][dataset] = np.nanmean(all_explore_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.figure(figsize=(4, 6))\n",
    "plt.plot([0, 1], [conditional_explore_en['normal_empty_low_energy'], conditional_explore_en['normal_empty_high_energy']], '-o', alpha=0.3, color='C0')\n",
    "plt.plot([0, 1], [conditional_explore_en['sparse_empty_low_energy'], conditional_explore_en['sparse_empty_high_energy']], '-o', alpha=0.3, color='C1')\n",
    "plt.plot([0, 1], [np.mean(conditional_explore_en['normal_empty_low_energy']), np.mean(conditional_explore_en['normal_empty_high_energy'])], '-o', linewidth=3, label='Dense', color='C0')\n",
    "plt.plot([0, 1], [np.mean(conditional_explore_en['sparse_empty_low_energy']), np.mean(conditional_explore_en['sparse_empty_high_energy'])], '-o', linewidth=3, label='Patchy', color='C1')\n",
    "plt.xticks([0, 1], ['Low', 'High'])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.ylabel('Exploratory energy output (a.u.)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prey and predator locations, triggered on actions\n",
    "%matplotlib qt\n",
    "\n",
    "agent_id = 6\n",
    "env = 'normal'\n",
    "lim = 17\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "dirs = get_dirs(env, env)\n",
    "print(f'Env: {env}, Agent: {agent_id+1}')\n",
    "t_prey, t_pred = get_trig_space(dirs[agent_id], lim=lim*10)\n",
    "ax.scatter(0, 0, s=50, c='k', label='Fish', marker='x')\n",
    "\n",
    "ax.scatter(t_prey[6][:, 1]/10, t_prey[6][:, 0]/10, s=2, alpha=0.05, c='b', label='Left J turn')\n",
    "ax.scatter(t_prey[5][:, 1]/10, t_prey[5][:, 0]/10, s=2, alpha=0.05, c='r', label='Right J turn')\n",
    "ax.scatter(t_prey[19][:, 1]/10, t_prey[19][:, 0]/10, s=2, alpha=0.05, c='g', label='Left HAT')\n",
    "ax.scatter(t_prey[18][:, 1]/10, t_prey[18][:, 0]/10, s=2, alpha=0.05, c='m', label='Right HAT')\n",
    "ax.scatter(t_prey[0][:, 1]/10, t_prey[0][:, 0]/10, s=2, alpha=0.05, c='c', label='Short Capture')\n",
    "ax.scatter(t_prey[1][:, 1]/10, t_prey[1][:, 0]/10, s=2, alpha=0.05, c='orange', label='Long Capture')\n",
    "\n",
    "ax.scatter(t_pred[4][:, 1]/10, t_pred[4][:, 0]/10, s=12, alpha=0.4, c='b', label='Left O-bend', marker='d')\n",
    "ax.scatter(t_pred[3][:, 1]/10, t_pred[3][:, 0]/10, s=12, alpha=0.4, c='r', label='Right O-bend', marker='d')\n",
    "ax.scatter(t_pred[14][:, 1]/10, t_pred[14][:, 0]/10, s=12, alpha=0.4, c='g', label='Left LLC', marker='d')\n",
    "ax.scatter(t_pred[13][:, 1]/10, t_pred[13][:, 0]/10, s=12, alpha=0.4, c='m', label='Right LLC', marker='d')\n",
    "ax.set_xlim(-lim, lim)\n",
    "ax.set_ylim(-lim, lim)\n",
    "# remove ticks\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "# plot a right angle to indicate scale\n",
    "corner_coord = -lim + 3\n",
    "ax.plot([corner_coord, corner_coord + 3], [corner_coord, corner_coord], '-k', linewidth=2)\n",
    "ax.plot([corner_coord, corner_coord], [corner_coord, corner_coord + 3], '-k', linewidth=2)\n",
    "\n",
    "# make aspect ratio equal\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# put the legend in a separate figure, with larger markers and alpha=1\n",
    "fig_legend, ax_legend = plt.subplots(1, 1, figsize=(3, 3))\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "ax_legend.legend(handles, labels, loc='center', frameon=False, markerscale=2)\n",
    "# change alpha to 1\n",
    "for handle in ax_legend.legend_.legendHandles:\n",
    "    handle.set_alpha(1)\n",
    "ax_legend.axis('off')\n",
    "fig_legend.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_episode('/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_5/logs/eval_sparse/logs_17.hdf5', prey_density=True)\n",
    "plot_episode('/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_5/logs/eval_normal/logs_20.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training env: normal, eval env: normal\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_1/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_2/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_3/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_4/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_5/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_6/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_7/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_8/logs/eval_normal/\n",
      "Processing training env: normal, eval env: sparse\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_1/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_2/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_3/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_4/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_5/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_6/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_7/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_normal_nohdf5/stage2_8/logs/eval_sparse/\n",
      "Processing training env: sparse, eval env: normal\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_1/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_2/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_3/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_4/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_5/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_6/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_7/logs/eval_normal/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_8/logs/eval_normal/\n",
      "Processing training env: sparse, eval env: sparse\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_1/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_2/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_3/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_4/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_5/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_6/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_7/logs/eval_sparse/\n",
      "Processing dir: /media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_sparse_nohdf5/stage2_8/logs/eval_sparse/\n"
     ]
    }
   ],
   "source": [
    "# get reward compositions\n",
    "combined_rewards = {}\n",
    "combined_durations = {}\n",
    "for training_env in ['normal', 'sparse']:\n",
    "    for eval_env in ['normal', 'sparse']:\n",
    "        print(f\"Processing training env: {training_env}, eval env: {eval_env}\")\n",
    "        dirs = get_dirs(training_env, eval_env)\n",
    "        all_rewards = {\n",
    "            'salt_reward': [],\n",
    "            'prey_reward': [],\n",
    "            'pred_reward': [],\n",
    "            'energy_reward': [],\n",
    "            'episode_return': []\n",
    "        }\n",
    "        all_durations = []\n",
    "        for dir in dirs:\n",
    "            print(f\"Processing dir: {dir}\")\n",
    "            dir_rewards, durations = get_reward_composition(dir)\n",
    "            for k in all_rewards.keys():\n",
    "                all_rewards[k].append(dir_rewards[k])\n",
    "            all_durations.append(durations)\n",
    "        for k in all_rewards.keys():\n",
    "            all_rewards[k] = np.array(all_rewards[k])\n",
    "        all_durations = np.array(all_durations)\n",
    "        combined_rewards[f'{training_env}_{eval_env}'] = all_rewards\n",
    "        combined_durations[f'{training_env}_{eval_env}'] = all_durations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_rewards['sparse_sparse']['prey_reward'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_type = 'prey_reward'\n",
    "ratio = 0.5\n",
    "\n",
    "normalized_combined_rewards = {}\n",
    "for key in combined_rewards.keys():\n",
    "    normalized_combined_rewards[key] = {}\n",
    "    for k in combined_rewards[key].keys():\n",
    "        normalized_combined_rewards[key][k] = combined_rewards[key][k] / combined_durations[key]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Episode return')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rewards_to_show = normalized_combined_rewards\n",
    "\n",
    "nn_mean = np.mean(rewards_to_show['normal_normal'][reward_type], axis=1)\n",
    "nn_sem = np.std(rewards_to_show['normal_normal'][reward_type], axis=1) / np.sqrt(rewards_to_show['normal_normal'][reward_type].shape[1])\n",
    "sn_mean = np.mean(rewards_to_show['sparse_normal'][reward_type], axis=1)\n",
    "sn_sem = np.std(rewards_to_show['sparse_normal'][reward_type], axis=1) / np.sqrt(rewards_to_show['sparse_normal'][reward_type].shape[1])\n",
    "ns_mean = np.mean(rewards_to_show['normal_sparse'][reward_type], axis=1)\n",
    "ns_sem = np.std(rewards_to_show['normal_sparse'][reward_type], axis=1) / np.sqrt(rewards_to_show['normal_sparse'][reward_type].shape[1])\n",
    "ss_mean = np.mean(rewards_to_show['sparse_sparse'][reward_type], axis=1)\n",
    "ss_sem = np.std(rewards_to_show['sparse_sparse'][reward_type], axis=1) / np.sqrt(rewards_to_show['sparse_sparse'][reward_type].shape[1])\n",
    "\n",
    "# make a line plot with error bars\n",
    "plt.figure(figsize=(4,6))\n",
    "for agent in range(8):\n",
    "    plt.errorbar([0, 1], [nn_mean[agent]*ratio, ns_mean[agent]*ratio], yerr=[nn_sem[agent]*ratio, ns_sem[agent]*ratio], fmt='-o', color='C0', alpha=0.5)\n",
    "    plt.errorbar([0, 1], [sn_mean[agent]*ratio, ss_mean[agent]*ratio], yerr=[sn_sem[agent]*ratio, ss_sem[agent]*ratio], fmt='-o', color='C1', alpha=0.5)\n",
    "plt.plot([0, 1], [np.mean(nn_mean)*ratio, np.mean(ns_mean)*ratio], '-o', color='C0', label='Trained normal')\n",
    "plt.plot([0, 1], [np.mean(sn_mean)*ratio, np.mean(ss_mean)*ratio], '-o', color='C1', label='Trained sparse')\n",
    "plt.xticks([0, 1], ['Dense', 'Patchy'])\n",
    "plt.ylabel('Episode return')\n",
    "# plt.title('Episode return by training and eval environment')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get streaks for normal and sparse envs\n",
    "\n",
    "dirs = get_dirs('sparse', 'empty_high_energy')\n",
    "\n",
    "mean_sparse_streaks = []\n",
    "sem_sparse_streaks = []\n",
    "for dir in dirs:\n",
    "    print(f\"Processing dir: {dir}\")\n",
    "    streaks, eligible_turns = get_turn_streaks(dir)\n",
    "    print(f\"Eligible turns: {eligible_turns}\")\n",
    "    mean_sparse_streaks.append(np.mean(streaks, axis=0))\n",
    "    sem_sparse_streaks.append(np.std(streaks, axis=0) / np.sqrt(streaks.shape[0]))\n",
    "mean_sparse_streaks = np.array(mean_sparse_streaks)\n",
    "sem_sparse_streaks = np.array(sem_sparse_streaks)\n",
    "\n",
    "dirs = get_dirs('normal', 'empty_high_energy')\n",
    "mean_normal_streaks = []\n",
    "sem_normal_streaks = []\n",
    "for dir in dirs:\n",
    "    print(f\"Processing dir: {dir}\")\n",
    "    streaks, eligible_turns = get_turn_streaks(dir)\n",
    "    print(f\"Eligible turns: {eligible_turns}\")\n",
    "    mean_normal_streaks.append(np.mean(streaks, axis=0))\n",
    "    sem_normal_streaks.append(np.std(streaks, axis=0) / np.sqrt(streaks.shape[0]))\n",
    "mean_normal_streaks = np.array(mean_normal_streaks)\n",
    "sem_normal_streaks = np.array(sem_normal_streaks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn chains plot\n",
    "step10_means_normal = np.zeros(len(dirs))\n",
    "step10_means_sparse = np.zeros(len(dirs))\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 3, 1)\n",
    "for dataset in range(len(dirs)):\n",
    "    step10_means_normal[dataset] = mean_normal_streaks[dataset][10]\n",
    "    plt.plot(mean_normal_streaks[dataset], '-o', label=f'Ag {dataset+1}', markersize=3)\n",
    "    plt.fill_between(np.arange(len(mean_normal_streaks[dataset])),\n",
    "                     mean_normal_streaks[dataset] - sem_normal_streaks[dataset],\n",
    "                     mean_normal_streaks[dataset] + sem_normal_streaks[dataset], alpha=0.3)\n",
    "\n",
    "plt.axhline(0, color='k', linestyle='--', label='Chance')\n",
    "plt.xlabel('Timestep after switch')\n",
    "plt.ylabel('Cumulative turn angle sign')\n",
    "plt.title('Dense Env')\n",
    "plt.ylim(-0.05, 1.5)\n",
    "# plt.legend(fontsize=8)\n",
    "plt.subplot(1, 3, 2)\n",
    "for dataset in range(len(dirs)):\n",
    "    step10_means_sparse[dataset] = mean_sparse_streaks[dataset][10]\n",
    "    plt.plot(mean_sparse_streaks[dataset], '-o', label=f'Ag {dataset+1}', markersize=3)\n",
    "    plt.fill_between(np.arange(len(mean_sparse_streaks[dataset])),\n",
    "                     mean_sparse_streaks[dataset] - sem_sparse_streaks[dataset],\n",
    "                     mean_sparse_streaks[dataset] + sem_sparse_streaks[dataset], alpha=0.3)\n",
    "plt.axhline(0, color='k', linestyle='--', label='Chance')\n",
    "plt.xlabel('Timestep after switch')\n",
    "plt.ylabel('Cumulative turn angle sign')\n",
    "plt.title('Patchy Env')\n",
    "plt.ylim(-0.05, 1.5)\n",
    "plt.legend(fontsize=8)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot([0, 1], [step10_means_normal, step10_means_sparse], '-o', label='Mean')\n",
    "plt.xticks([0, 1], ['Dense', 'Patchy'])\n",
    "plt.title('Cumulative turn angle at t=10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = get_dirs('normal', 'normal')\n",
    "action_names, action_colors, action_types, action_is_turn, _ = get_actions('../actions_all_bouts_with_null.h5')\n",
    "agents = [5, 6]\n",
    "fig, ax = plt.subplots(len(agents), 2, figsize=(15, 20))\n",
    "for ii, agent_id in enumerate(agents):\n",
    "    avoid_seq, caught_seq, pred_seq, consume_seq = get_sequences(dirs[agent_id], steps_pre=15, steps_post=15)\n",
    "    timebase = np.arange(-15, 15)\n",
    "    for ss, seq in enumerate([pred_seq, consume_seq]):\n",
    "        for action_name, action_id in zip(action_names, range(len(action_names))):\n",
    "            if '_L' in action_name:\n",
    "                continue\n",
    "            elif '_R' in action_name:\n",
    "                this_name = action_name.split('_')[0]\n",
    "                action_props = np.mean((seq == action_id) | (seq == action_id + 1), axis=0)\n",
    "            else:\n",
    "                this_name = action_name\n",
    "                action_props = np.mean(seq == action_id, axis=0)\n",
    "            this_color = action_colors[action_id]\n",
    "            if np.max(action_props) < 0.1:\n",
    "                continue\n",
    "            ax[ii, ss].plot(timebase, action_props, '-o', label=this_name, color=this_color, markersize=3, linewidth=2)\n",
    "            # vertical line at time 0\n",
    "        ax[ii, ss].axvline(0, color='k', linestyle='--', linewidth=1)\n",
    "        ax[ii, ss].set_ylim(0, 0.8)\n",
    "        ax[ii, ss].set_xlim(-15, 15)\n",
    "        ax[ii, ss].set_xlabel('Timestep')\n",
    "        ax[ii, ss].set_ylabel('Proportion of actions')\n",
    "        \n",
    "        \n",
    "        #plt.legend(fontsize=6, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        # ax[0, ss].set_xlabel('Timestep relative to avoid event')\n",
    "        # ax[0, ss].set_ylabel('Proportion of actions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avoid_seq.shape, caught_seq.shape, pred_seq.shape, consume_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def smooth(scalars: list[float], weight: float) -> list[float]:\n",
    "    \"\"\"\n",
    "    EMA implementation according to\n",
    "    https://github.com/tensorflow/tensorboard/blob/34877f15153e1a2087316b9952c931807a122aa7/tensorboard/components/vz_line_chart2/line-chart.ts#L699\n",
    "    \"\"\"\n",
    "    last = 0\n",
    "    smoothed = []\n",
    "    num_acc = 0\n",
    "    for next_val in scalars:\n",
    "        last = last * weight + (1 - weight) * next_val\n",
    "        num_acc += 1\n",
    "        # de-bias\n",
    "        debias_weight = 1\n",
    "        if weight != 1:\n",
    "            debias_weight = 1 - weight**num_acc\n",
    "        smoothed_val = last / debias_weight\n",
    "        smoothed.append(smoothed_val)\n",
    "\n",
    "    return smoothed\n",
    "training_episode_return_normal = []\n",
    "training_episode_return_sparse = []\n",
    "\n",
    "training_env = 'normal'\n",
    "dirs = [f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_1/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_2/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_3/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_4/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_5/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_6/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_7/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_8/logs/',\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "for dir in dirs:\n",
    "    training_logs_dir = dir + 'training/actor_0/'\n",
    "    events_file = glob.glob(training_logs_dir + 'events*')[0]\n",
    "    training_episode_return_normal.append(read_events_file(events_file, 'actor/EpisodeReturn'))\n",
    "\n",
    "training_env = 'sparse'\n",
    "\n",
    "dirs = [f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_1/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_2/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_3/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_4/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_5/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_6/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_7/logs/',\n",
    "        f'/media/gf/DBIO_Bianco_Lab8/temp_data_storage/gcp_output/stage2_{training_env}_nohdf5/stage2_8/logs/',\n",
    "\n",
    "]\n",
    "\n",
    "for dir in dirs:\n",
    "    training_logs_dir = dir + 'training/actor_0/'\n",
    "    events_file = glob.glob(training_logs_dir + 'events*')[0]\n",
    "    training_episode_return_sparse.append(read_events_file(events_file, 'actor/EpisodeReturn'))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "all_normal = []\n",
    "all_sparse = []\n",
    "for agent in range(len(training_episode_return_normal)):\n",
    "    smoothed_normal = smooth(training_episode_return_normal[agent][:, 1], 0.99)[10:]\n",
    "    smoothed_sparse = smooth(training_episode_return_sparse[agent][:, 1], 0.99)[10:]\n",
    "    # fill with nans up to length 5000\n",
    "    if len(smoothed_normal) < 5000:\n",
    "        smoothed_normal = smoothed_normal + [np.nan] * (5000 - len(smoothed_normal))\n",
    "    if len(smoothed_sparse) < 5000:\n",
    "        smoothed_sparse = smoothed_sparse + [np.nan] * (5000 - len(smoothed_sparse))\n",
    "    steps_normal = training_episode_return_normal[agent][10:, 0]\n",
    "    steps_sparse = training_episode_return_sparse[agent][10:, 0]\n",
    "    if len(steps_normal) < 5000:\n",
    "        steps_normal = np.concatenate((steps_normal, np.full(5000 - len(steps_normal), np.nan)))\n",
    "    if len(steps_sparse) < 5000:\n",
    "        steps_sparse = np.concatenate((steps_sparse, np.full(5000 - len(steps_sparse), np.nan)))\n",
    "    all_normal.append(np.array(smoothed_normal)[:5000])\n",
    "    all_sparse.append(np.array(smoothed_sparse)[:5000])\n",
    "    plt.plot(steps_normal/1e6, smoothed_normal, color='C0', alpha=0.3)\n",
    "    plt.plot(steps_sparse/1e6, smoothed_sparse, color='C1', alpha=0.3)\n",
    "\n",
    "all_normal = np.array(all_normal)\n",
    "all_sparse = np.array(all_sparse)\n",
    "mean_normal = np.nanmean(all_normal, axis=0)\n",
    "mean_sparse = np.nanmean(all_sparse, axis=0)\n",
    "plt.plot(steps_normal[:5000]/1e6, mean_normal, color='C0', label='Trained dense', linewidth=3)\n",
    "plt.plot(steps_sparse[:5000]/1e6, mean_sparse, color='C1', label='Trained patchy', linewidth=3)\n",
    "plt.xlabel('Training steps (millions)')\n",
    "plt.ylabel('Episode return')\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_sparse.shape, steps_normal.shape, mean_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(all_normal[i]) for i in range(len(all_normal))], [len(all_sparse[i]) for i in range(len(all_sparse))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for ds in range(8):\n",
    "    smoothed = smooth(training_episode_return[ds][:, 1], 0.99)[10:]\n",
    "    plt.plot(training_episode_return[ds][10:, 0]/1e6, smoothed, label=f'Dir {ds+1}')\n",
    "plt.xlabel('Actor Steps (millions)')\n",
    "plt.ylabel('Episode Return (EMA smoothed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mean_streaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_avoid = np.concatenate(pre_avoid_seq)\n",
    "# plt.imshow((pre_avoid==3) | (pre_avoid==4))\n",
    "%matplotlib qt\n",
    "plt.figure()\n",
    "from matplotlib.colors import ListedColormap\n",
    "pre_con = np.concatenate(pre_consume_seq)\n",
    "pre_avoid = np.concatenate(pre_avoid_seq)\n",
    "my_cmap = ListedColormap(np.array(action_colors))\n",
    "time_base = np.arange(pre_con.shape[1]) - 20\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(pre_con, aspect='auto', cmap=my_cmap, extent=[time_base[0], time_base[-1], 0, pre_con.shape[0]], interpolation='nearest')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(time_base, np.mean((pre_con==3) | (pre_con==4), axis=0), label='O-bend', color=action_colors[3])\n",
    "plt.plot(time_base, np.mean((pre_con==5) | (pre_con==6), axis=0), label='J turn', color=action_colors[5])\n",
    "plt.plot(time_base, np.mean((pre_con==0) | (pre_con==1), axis=0), label='Capture', color=action_colors[0])\n",
    "plt.plot(time_base, np.mean((pre_con==18) | (pre_con==19), axis=0), label='HAT', color=action_colors[18])\n",
    "plt.xlim([-20, 5])\n",
    "plt.legend()\n",
    "plt.xlabel('Time steps relative to prey capture')\n",
    "plt.ylabel('Proportion of actions')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(pre_avoid, aspect='auto', cmap=my_cmap, extent=[time_base[0], time_base[-1], 0, pre_avoid.shape[0]], interpolation='nearest')\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(time_base, np.mean((pre_avoid==3) | (pre_avoid==4), axis=0), label='O-bend', color=action_colors[3])\n",
    "plt.plot(time_base, np.mean((pre_avoid==5) | (pre_avoid==6), axis=0), label='J turn', color=action_colors[5])\n",
    "plt.plot(time_base, np.mean((pre_avoid==0) | (pre_avoid==1), axis=0), label='Capture', color=action_colors[0])\n",
    "plt.plot(time_base, np.mean((pre_avoid==18) | (pre_avoid==19), axis=0), label='HAT', color=action_colors[18])\n",
    "plt.xlim([-20, 5])\n",
    "plt.xlabel('Time steps relative to predator avoidance')\n",
    "plt.ylabel('Proportion of actions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq0 = np.concatenate(q0, axis=0)/10\n",
    "qq1 = np.concatenate(q1, axis=0)/10\n",
    "qq3 = np.concatenate(q3, axis=0)/10\n",
    "qq4 = np.concatenate(q4, axis=0)/10\n",
    "qq5 = np.concatenate(q5, axis=0)/10\n",
    "qq6 = np.concatenate(q6, axis=0)/10\n",
    "qq18 = np.concatenate(q18, axis=0)/10\n",
    "qq19 = np.concatenate(q19, axis=0)/10\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(qq0[:, :, 1], qq0[:, :, 0], s=2, alpha=0.05,color='orange', label='Short Capture')\n",
    "plt.scatter(qq1[:, :, 1], qq1[:, :, 0], s=2, alpha=0.05,color='c', label='Long Capture')\n",
    "\n",
    "plt.scatter(qq5[:, :, 1], qq5[:, :, 0], s=2, alpha=0.05,color='r', label='Right J turn')\n",
    "plt.scatter(qq6[:, :, 1], qq6[:, :, 0], s=2, alpha=0.05,color='b', label='Left J turn')\n",
    "\n",
    "plt.scatter(qq3[:, :, 1], qq3[:, :, 0], s=8, alpha=0.3,color='r', label='Right O-bend', marker='d')\n",
    "plt.scatter(qq4[:, :, 1], qq4[:, :, 0], s=8, alpha=0.3,color='b', label='Left O-bend', marker='d')\n",
    "\n",
    "plt.scatter(qq18[:, :, 1], qq18[:, :, 0], s=2, alpha=0.05,color='m', label='Right HAT')\n",
    "plt.scatter(qq19[:, :, 1], qq19[:, :, 0], s=2, alpha=0.05,color='g', label='Left HAT')\n",
    "plt.scatter(0, 0, s=100, c='k', marker='x')\n",
    "# plt.axis('equal')\n",
    "\n",
    "plt.xlim(-17, 17)\n",
    "plt.ylim(-10, 15)\n",
    "plt.xlabel('Object X (fish frame, mm)')\n",
    "plt.ylabel('Object Y (fish frame, mm)')\n",
    "# plt.legend(bbox_to_anchor=(1.02, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_types = []\n",
    "ids = []\n",
    "with h5py.File('../actions_all_bouts_with_null.h5', 'r') as f:\n",
    "    for group_name in f.keys():\n",
    "        group = f[group_name]\n",
    "        action = {\n",
    "                    'name': group_name,\n",
    "                    'mean': group['mean'][:],\n",
    "                    'cov': group['cov'][:],\n",
    "                    'is_turn': group.attrs['is_turn'],\n",
    "                    'is_capture': group.attrs['is_capture'],\n",
    "                    'color': group.attrs['color']\n",
    "            }\n",
    "        if '_L' in action['name']:\n",
    "            action['color'] *= 1.1\n",
    "        if '_R' in action['name']:\n",
    "            action['color'] *= 0.9\n",
    "        action['color'] = np.clip(action['color'], 0, 1)\n",
    "        ids.append(group.attrs['id'])\n",
    "        action_types.append(action)\n",
    "        # sort actions by id\n",
    "action_types = [x for _, x in sorted(zip(ids, action_types), key=lambda pair: pair[0])]\n",
    "action_names = [action_types[i]['name'] for i in range(len(action_types))]\n",
    "action_colors = [action_types[i]['color'] for i in range(len(action_types))]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "a_prop = np.array(actions_proportion)\n",
    "# show as stacked bar plot\n",
    "labels = [str(i) for i in range(a_prop.shape[1])]\n",
    "x = np.arange(a_prop.shape[0])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "bottom = np.zeros(a_prop.shape[0])\n",
    "# set a color sequence\n",
    "\n",
    "for i in range(a_prop.shape[1]):\n",
    "    ax.bar(x, a_prop[:, i], bottom=bottom, label=action_names[i], color=action_colors[i])\n",
    "    bottom += a_prop[:, i]\n",
    "ax.set_xlabel('Agent')\n",
    "ax.set_ylabel('Proportion')\n",
    "ax.set_title('Action Proportions Across Agents')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], loc='upper left', bbox_to_anchor=(1.02, 1), fontsize=8)\n",
    "\n",
    "bottom = np.zeros(a_prop.shape[0])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "for i, reward_type in enumerate(['prey_reward', 'pred_reward']):\n",
    "    ax.bar(x, np.mean(all_rewards[reward_type], axis=1), label=reward_type, bottom=bottom)\n",
    "    bottom += np.mean(all_rewards[reward_type], axis=1)\n",
    "bottom = np.zeros(a_prop.shape[0])\n",
    "for i, reward_type in enumerate(['salt_reward', 'energy_reward']):\n",
    "    ax.bar(x, np.mean(all_rewards[reward_type], axis=1), label=reward_type, bottom=bottom)\n",
    "    bottom += np.mean(all_rewards[reward_type], axis=1)\n",
    "\n",
    "# make a line plot of episode return on the same axis\n",
    "#ax.plot(x, np.mean(all_rewards['episode_return'], axis=1), color='k', marker='o', label='Total Return')\n",
    "# plot error bars for episode return\n",
    "ax.errorbar(x, np.mean(all_rewards['episode_return'], axis=1), yerr=np.std(all_rewards['episode_return'], axis=1), color='k', fmt='o', capsize=3, label='Total Return')\n",
    "# ax[1].scatter(x, np.mean(-1*all_rewards['energy_reward'], axis=1), color='k', marker='o', label='Energy Cost')\n",
    "# ax[1].scatter(x, np.mean(-1*all_rewards['salt_reward'], axis=1), color='grey', marker='o', label='Salt Cost')\n",
    "\n",
    "ax.set_xlabel('Agent')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "# bold horizontal line at y=0\n",
    "ax.axhline(0, color='k', linewidth=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
